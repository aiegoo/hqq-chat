{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyOYouP1+MvorWjMw0TTfrGi",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "7441c9691c5a4e558a4448433c23f2df": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_77f959601ec94fb4b50c9dadd5882ff6",
              "IPY_MODEL_de70414db49b4f33a4df3f7dbd2d166b",
              "IPY_MODEL_8b54639a40a247a4b4eb6e0f47534816"
            ],
            "layout": "IPY_MODEL_eb2c8d7b40734e49a97ed3e46fb111bf"
          }
        },
        "77f959601ec94fb4b50c9dadd5882ff6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e7f02ee0abc84aae9d2b0355025a96e3",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_e926cf5a553c494fb9680329cd29711c",
            "value": "Downloadingâ€‡builderâ€‡script:â€‡"
          }
        },
        "de70414db49b4f33a4df3f7dbd2d166b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d518939d43774042b40c015400977c8e",
            "max": 1652,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_d0a97e83cb8b4ab99e4864de8ca637ce",
            "value": 1652
          }
        },
        "8b54639a40a247a4b4eb6e0f47534816": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_cedad6e11533452890b864c14cd0d890",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_b516ed3540764c0c99a59153b6029bff",
            "value": "â€‡4.21k/?â€‡[00:00&lt;00:00,â€‡232kB/s]"
          }
        },
        "eb2c8d7b40734e49a97ed3e46fb111bf": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e7f02ee0abc84aae9d2b0355025a96e3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e926cf5a553c494fb9680329cd29711c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d518939d43774042b40c015400977c8e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d0a97e83cb8b4ab99e4864de8ca637ce": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "cedad6e11533452890b864c14cd0d890": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b516ed3540764c0c99a59153b6029bff": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aiegoo/hqq-chat/blob/master/NLP_QnA2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1XZw6ysCmvI2"
      },
      "outputs": [],
      "source": [
        "!pip install datasets"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import datasets\n",
        "accuracy_metric = datasets.load_metric('accuracy')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 173,
          "referenced_widgets": [
            "7441c9691c5a4e558a4448433c23f2df",
            "77f959601ec94fb4b50c9dadd5882ff6",
            "de70414db49b4f33a4df3f7dbd2d166b",
            "8b54639a40a247a4b4eb6e0f47534816",
            "eb2c8d7b40734e49a97ed3e46fb111bf",
            "e7f02ee0abc84aae9d2b0355025a96e3",
            "e926cf5a553c494fb9680329cd29711c",
            "d518939d43774042b40c015400977c8e",
            "d0a97e83cb8b4ab99e4864de8ca637ce",
            "cedad6e11533452890b864c14cd0d890",
            "b516ed3540764c0c99a59153b6029bff"
          ]
        },
        "id": "kvh69w9um6hN",
        "outputId": "f5c7c64d-9e57-4629-c77b-d04ad3054ab5"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-2-b830202e8b6a>:2: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library ðŸ¤— Evaluate: https://huggingface.co/docs/evaluate\n",
            "  accuracy_metric = datasets.load_metric('accuracy')\n",
            "/usr/local/lib/python3.10/dist-packages/datasets/load.py:759: FutureWarning: The repository for accuracy contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.19.0/metrics/accuracy/accuracy.py\n",
            "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
            "Passing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading builder script:   0%|          | 0.00/1.65k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "7441c9691c5a4e558a4448433c23f2df"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "results = accuracy_metric.compute(references=[0, 1], predictions=[0, 1])"
      ],
      "metadata": {
        "id": "VDVq8EL_nR0r"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "results"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9CciQbCwnbCy",
        "outputId": "a574a5c0-e814-4245-a2e4-2144812dbc32"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'accuracy': 1.0}"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "results = accuracy_metric.compute(references=[0, 1, 2, 0, 1, 2], predictions=[0, 1, 1, 2, 1, 0])"
      ],
      "metadata": {
        "id": "GhyOPbwFncEc"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "results"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x1qi1kMznjz-",
        "outputId": "65b905bb-167a-4607-dce2-79105d4de2c2"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'accuracy': 0.5}"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "results = accuracy_metric.compute(references=[0, 1, 2, 0, 1, 2], predictions=[0, 1, 1, 2, 1, 0], normalize=False)"
      ],
      "metadata": {
        "id": "M7k5WY3-nk8h"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "results"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_vmHIqNYnvee",
        "outputId": "29a4fc93-0f5a-4050-f0bb-01b78e55ff57"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'accuracy': 3.0}"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "results = accuracy_metric.compute(references=[0, 1, 2, 0, 1, 2], predictions=[0, 1, 1, 2, 1, 0], sample_weight=[0.5, 2, 0.7, 0.5, 9, 0.4])"
      ],
      "metadata": {
        "id": "yTL3R399nxAY"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "results"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bRLuMF64oB6b",
        "outputId": "7099381e-5f53-472d-dc2d-2f486ab7a917"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'accuracy': 0.8778625954198473}"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**LLM Mixtral**"
      ],
      "metadata": {
        "id": "Pw92NMPXpreY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers torch"
      ],
      "metadata": {
        "id": "N_bJ8cQlosif"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForCausalLM, AutoTokenizer"
      ],
      "metadata": {
        "id": "7EChcj7Muvd_"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def calculate_accuracy(prompts, references, model, tokenizer):\n",
        "    total_prompts = len(prompts)\n",
        "    correct_predictions = 0\n",
        "\n",
        "    for prompt, reference in zip(prompts, references):\n",
        "        # Map numerical prompt to textual representation\n",
        "        textual_prompt = numerical_to_text_prompt[prompt]\n",
        "\n",
        "        # Tokenize the textual prompt with padding\n",
        "        inputs = tokenizer(textual_prompt, return_tensors=\"pt\", padding=True, truncation=True)\n",
        "        input_ids = inputs.input_ids.to(model.device)\n",
        "\n",
        "        # Generate a response using Mistral\n",
        "        generated_ids = model.generate(input_ids, max_length=50, num_return_sequences=1)\n",
        "        generated_response = tokenizer.decode(generated_ids[0], skip_special_tokens=True)\n",
        "\n",
        "        # Map numerical reference to textual representation\n",
        "        textual_reference = numerical_to_text_response[reference]\n",
        "\n",
        "        # Check if the generated response matches the reference\n",
        "        if generated_response == textual_reference:\n",
        "            correct_predictions += 1\n",
        "\n",
        "    # Calculate accuracy\n",
        "    accuracy = (correct_predictions / total_prompts) * 100\n",
        "    return accuracy\n",
        "\n",
        "# Define mapping from numerical values to textual prompts/responses\n",
        "numerical_to_text_prompt = {\n",
        "    0: \"A for Apple\",\n",
        "    1: \"B for Banana\"\n",
        "}\n",
        "\n",
        "numerical_to_text_response = {\n",
        "    0: \"A for Apple\",\n",
        "    1: \"B for Banana\"\n",
        "}\n",
        "\n",
        "# Load Mistral model and tokenizer\n",
        "model = AutoModelForCausalLM.from_pretrained(\"microsoft/DialoGPT-small\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/DialoGPT-small\")\n",
        "\n",
        "# Set padding token in the tokenizer\n",
        "tokenizer.add_special_tokens({'pad_token': '[PAD]'})  # Add a new padding token\n",
        "\n",
        "# Define numerical prompts and corresponding references\n",
        "prompts = [0, 1]  # Numerical prompts\n",
        "references = [0, 1]  # Numerical references\n",
        "\n",
        "# Calculate accuracy using Mistral model\n",
        "accuracy = calculate_accuracy(prompts, references, model, tokenizer)\n",
        "print(f\"Accuracy: {accuracy:.2f}%\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aPacMWAmwsqu",
        "outputId": "f39b0d7a-042f-4b0b-dff6-3f36598df8d2"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:89: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n",
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 50.00%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_accuracy(prompts, references, model, tokenizer):\n",
        "    total_prompts = len(prompts)\n",
        "    correct_predictions = 0\n",
        "\n",
        "    for prompt, reference in zip(prompts, references):\n",
        "        # Map numerical prompt to textual representation\n",
        "        textual_prompt = numerical_to_text_prompt[prompt]\n",
        "\n",
        "        # Tokenize the textual prompt with padding\n",
        "        inputs = tokenizer(textual_prompt, return_tensors=\"pt\", padding=True, truncation=True)\n",
        "        input_ids = inputs.input_ids.to(model.device)\n",
        "\n",
        "        # Generate a response using Mistral\n",
        "        generated_ids = model.generate(input_ids, max_length=50, num_return_sequences=1)\n",
        "        generated_response = tokenizer.decode(generated_ids[0], skip_special_tokens=True)\n",
        "\n",
        "        # Map numerical reference to textual representation\n",
        "        textual_reference = numerical_to_text_response[reference]\n",
        "\n",
        "        # Check if the generated response matches the reference\n",
        "        if generated_response == textual_reference:\n",
        "            correct_predictions += 1\n",
        "\n",
        "    # Calculate accuracy\n",
        "    accuracy = (correct_predictions / total_prompts) * 100\n",
        "    return accuracy\n",
        "\n",
        "# Define mapping from numerical values to textual prompts/responses\n",
        "numerical_to_text_prompt = {\n",
        "    0: \"A for Apple\",\n",
        "    1: \"B for Banana\",\n",
        "    2: \"C for Cherry\"\n",
        "\n",
        "}\n",
        "\n",
        "numerical_to_text_response = {\n",
        "    0: \"A for Apple\",\n",
        "    1: \"B for Banana\",\n",
        "    2: \"C for Cherry\"\n",
        "}\n",
        "\n",
        "# Load Mistral model and tokenizer\n",
        "model = AutoModelForCausalLM.from_pretrained(\"microsoft/DialoGPT-small\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/DialoGPT-small\")\n",
        "\n",
        "# Set padding token in the tokenizer\n",
        "tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
        "accuracy = calculate_accuracy([0, 1, 2, 0, 1, 2], [0, 1, 1, 2, 1, 0], model, tokenizer)\n",
        "print(f\"Accuracy: {accuracy:.2f}%\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KB-SILztyQ0E",
        "outputId": "ffa3b1de-e3d0-4faf-dc59-6d9118ec9792"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 33.33%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for prompt, reference in zip(prompts, references):\n",
        "    textual_prompt = numerical_to_text_prompt[prompt]\n",
        "    inputs = tokenizer(textual_prompt, return_tensors=\"pt\", padding=True, truncation=True)\n",
        "    input_ids = inputs.input_ids.to(model.device)\n",
        "\n",
        "    # Generate a response using Mistral\n",
        "    generated_ids = model.generate(input_ids, max_length=50, num_return_sequences=1)\n",
        "    generated_response = tokenizer.decode(generated_ids[0], skip_special_tokens=True)\n",
        "\n",
        "    print(f\"Prompt: {textual_prompt}\")\n",
        "    print(f\"Expected Response: {numerical_to_text_response[reference]}\")\n",
        "    print(f\"Generated Response: {generated_response}\\n\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mzDeNmvZqJrP",
        "outputId": "c8f4d6a6-58f8-41a8-d9f1-33a47775c95b"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prompt: A for Apple\n",
            "Expected Response: A for Apple\n",
            "Generated Response: A for Apple Watch\n",
            "\n",
            "Prompt: B for Banana\n",
            "Expected Response: B for Banana\n",
            "Generated Response: B for Banana\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "accuracy = calculate_accuracy([0, 1, 2, 0, 1, 2], [0, 1, 1, 2, 1, 0], model, tokenizer)\n",
        "print(f\"Accuracy: {accuracy:.2f}%\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rQdKLEUCsKfn",
        "outputId": "d3ec00b2-4098-4d28-d4ff-67cb2f8cf385"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 33.33%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "4QnEjQyiQ9lj"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Hsl8v3NSQ887"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "from sklearn.metrics import f1_score, accuracy_score"
      ],
      "metadata": {
        "id": "AAu4P-6jsKc8"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_response(prompt, model, tokenizer, max_length=50):\n",
        "    input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids.to(model.device)\n",
        "    generated_ids = model.generate(input_ids, max_length=max_length, num_return_sequences=1)\n",
        "    generated_response = tokenizer.decode(generated_ids[0], skip_special_tokens=True)\n",
        "    return generated_response"
      ],
      "metadata": {
        "id": "MJQt0vuuG3vi"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_accuracy(prompts, references, model, tokenizer):\n",
        "    total_prompts = len(prompts)\n",
        "    correct_predictions = 0\n",
        "\n",
        "    for prompt, reference in zip(prompts, references):\n",
        "        # Map numerical prompt to textual representation\n",
        "        textual_prompt = numerical_to_text_prompt[prompt]\n",
        "\n",
        "        # Generate a response using Mistral\n",
        "        generated_response = generate_response(textual_prompt, model, tokenizer)\n",
        "\n",
        "        # Map numerical reference to textual representation\n",
        "        textual_reference = numerical_to_text_response[reference]\n",
        "\n",
        "        # Check if the generated response matches the reference\n",
        "        if generated_response == textual_reference:\n",
        "            correct_predictions += 1\n",
        "\n",
        "    # Calculate accuracy after the loop\n",
        "    accuracy = (correct_predictions / total_prompts) * 100\n",
        "    return accuracy\n"
      ],
      "metadata": {
        "id": "7bkniFUYMWbo"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import f1_score\n",
        "\n",
        "def calculate_f1_score(prompts, references, model, tokenizer):\n",
        "    all_predicted = []\n",
        "    all_references = []\n",
        "\n",
        "    for prompt, reference in zip(prompts, references):\n",
        "        # Map numerical prompt to textual representation\n",
        "        textual_prompt = numerical_to_text_prompt[prompt]\n",
        "\n",
        "        # Generate a response using Mistral\n",
        "        generated_response = generate_response(textual_prompt, model, tokenizer)\n",
        "\n",
        "        # Map numerical reference to textual representation\n",
        "        textual_reference = numerical_to_text_response[reference]\n",
        "\n",
        "        all_predicted.append(generated_response)\n",
        "        all_references.append(textual_reference)\n",
        "\n",
        "    # Calculate F1 score\n",
        "    f1 = f1_score(all_references, all_predicted, average='micro')\n",
        "    return f1\n"
      ],
      "metadata": {
        "id": "6eDVXbkeMWYX"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "def calculate_mape(prompts, references, model, tokenizer):\n",
        "    total_prompts = len(prompts)\n",
        "    total_error = 0.0\n",
        "\n",
        "    for prompt, reference in zip(prompts, references):\n",
        "        # Map numerical prompt to textual representation\n",
        "        textual_prompt = numerical_to_text_prompt[prompt]\n",
        "\n",
        "        # Generate a response using Mistral\n",
        "        generated_response = generate_response(textual_prompt, model, tokenizer)\n",
        "\n",
        "        # Convert generated response to numerical value (if applicable)\n",
        "        try:\n",
        "            generated_value = int(generated_response)  # Assuming the response is a numerical value\n",
        "        except ValueError:\n",
        "            continue  # Skip if the response cannot be converted to an integer\n",
        "\n",
        "        # Calculate absolute percentage error\n",
        "        absolute_error = np.abs((generated_value - reference) / reference)\n",
        "        total_error += absolute_error\n",
        "\n",
        "    # Calculate mean absolute percentage error (MAPE)\n",
        "    if total_prompts > 0:\n",
        "        mape = (total_error / total_prompts) * 100\n",
        "    else:\n",
        "        mape = 0.0\n",
        "\n",
        "    return mape\n"
      ],
      "metadata": {
        "id": "lyzPYT4tLQfu"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = AutoModelForCausalLM.from_pretrained(\"microsoft/DialoGPT-small\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/DialoGPT-small\")\n",
        "tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
        "prompts = [0, 1]  # Numerical prompts\n",
        "references = [0, 1]  # Numerical references"
      ],
      "metadata": {
        "id": "aRpmAKy0PBjh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "accuracy = calculate_accuracy(prompts, references, model, tokenizer)\n",
        "f1_score = calculate_f1_score(prompts, references, model, tokenizer)\n",
        "mape = calculate_mape(prompts, references, model, tokenizer)\n",
        "perplexity = calculate_perplexity(prompts, model, tokenizer)\n",
        "print(f\"Accuracy: {accuracy:.2f}%\")\n",
        "print(f\"F1 Score: {f1_score:.2f}\")\n",
        "print(f\"MAPE: {mape:.2f}\")\n",
        "print(f\"Perplexity: {perplexity:.2f}\")"
      ],
      "metadata": {
        "id": "RFwxWWBvPfqi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "q5Gb1HguVOxF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "> import llm model mixtral\n",
        "> run the metrics on them\n",
        "would a single fucntion calculate_metrics would be good?\n",
        "or a differenct function for all below? one by one.. so different separate\n",
        "F1\n",
        "Accuracy\n",
        "MAPE\n",
        "Perplxity\n"
      ],
      "metadata": {
        "id": "puwv9gS1o1ej"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "from sklearn.metrics import f1_score\n",
        "\n",
        "# Define numerical prompts and corresponding textual representations\n",
        "numerical_to_text_prompt = {\n",
        "    0: \"A for Apple\",\n",
        "    1: \"B for Banana\"\n",
        "}\n",
        "\n",
        "numerical_to_text_response = {\n",
        "    0: \"A for Apple\",\n",
        "    1: \"B for Banana\"\n",
        "}\n",
        "\n",
        "def generate_response(prompt, model, tokenizer, max_length=50):\n",
        "    input_ids = tokenizer(prompt, return_tensors=\"pt\", padding=True, truncation=True)['input_ids'].to(model.device)\n",
        "    generated_ids = model.generate(input_ids, max_length=max_length, num_return_sequences=1)\n",
        "    generated_response = tokenizer.decode(generated_ids[0], skip_special_tokens=True)\n",
        "    return generated_response\n",
        "\n",
        "def calculate_accuracy(prompts, references, model, tokenizer):\n",
        "    total_prompts = len(prompts)\n",
        "    correct_predictions = 0\n",
        "\n",
        "    for prompt, reference in zip(prompts, references):\n",
        "        textual_prompt = numerical_to_text_prompt[prompt]\n",
        "        generated_response = generate_response(textual_prompt, model, tokenizer)\n",
        "        textual_reference = numerical_to_text_response[reference]\n",
        "\n",
        "        if generated_response == textual_reference:\n",
        "            correct_predictions += 1\n",
        "\n",
        "    accuracy = (correct_predictions / total_prompts) * 100\n",
        "    return accuracy\n",
        "\n",
        "def calculate_f1_score(prompts, references, model, tokenizer):\n",
        "    all_predicted = []\n",
        "    all_references = []\n",
        "\n",
        "    for prompt, reference in zip(prompts, references):\n",
        "        textual_prompt = numerical_to_text_prompt[prompt]\n",
        "        generated_response = generate_response(textual_prompt, model, tokenizer)\n",
        "        textual_reference = numerical_to_text_response[reference]\n",
        "\n",
        "        all_predicted.append(generated_response)\n",
        "        all_references.append(textual_reference)\n",
        "\n",
        "    f1 = f1_score(all_references, all_predicted, average='micro')\n",
        "    return f1\n",
        "\n",
        "\n",
        "def parse_generated_response(generated_response):\n",
        "    try:\n",
        "        # Split the generated response by \":\" and extract the value part\n",
        "        value_part = generated_response.split(\":\")[1].strip()\n",
        "        # Convert the extracted value to float\n",
        "        parsed_value = float(value_part)\n",
        "    except (IndexError, ValueError):\n",
        "        # Handle the case where the generated response does not match the expected format\n",
        "        parsed_value = 0.0  # Default value or appropriate error handling\n",
        "    return parsed_value\n",
        "\n",
        "\n",
        "def parse_reference_response(reference_response):\n",
        "    try:\n",
        "        # Split the reference_response by \":\" and extract the second part\n",
        "        parts = reference_response.split(\":\")\n",
        "        if len(parts) > 1:\n",
        "            value_part = parts[1].strip()\n",
        "            # Convert the extracted value to float\n",
        "            parsed_value = float(value_part)\n",
        "        else:\n",
        "            parsed_value = 0.0  # Default value or appropriate error handling if the format is unexpected\n",
        "    except (IndexError, ValueError):\n",
        "        parsed_value = 0.0  # Default value or appropriate error handling in case of exceptions\n",
        "    return parsed_value\n",
        "\n",
        "\n",
        "def calculate_mape(prompts, references, model, tokenizer):\n",
        "    total_prompts = len(prompts)\n",
        "    total_error = 0\n",
        "\n",
        "    for prompt, reference in zip(prompts, references):\n",
        "        textual_prompt = numerical_to_text_prompt[prompt]\n",
        "        generated_response = generate_response(textual_prompt, model, tokenizer)\n",
        "        predicted_value = parse_generated_response(generated_response)\n",
        "\n",
        "        # Retrieve the textual reference corresponding to the numerical reference\n",
        "        textual_reference = numerical_to_text_response[reference]\n",
        "\n",
        "        # Parse the true value from the textual reference\n",
        "        true_value = parse_reference_response(textual_reference)\n",
        "\n",
        "        if true_value != 0:  # Check if true_value is not zero to avoid division by zero\n",
        "            absolute_error = np.abs((predicted_value - true_value) / true_value)\n",
        "            total_error += absolute_error\n",
        "\n",
        "    # Calculate MAPE only if there are non-zero true values\n",
        "    if total_prompts > 0:\n",
        "        mape = total_error / total_prompts\n",
        "    else:\n",
        "        mape = 0.0  # Default value if total_prompts is zero\n",
        "\n",
        "    return mape\n",
        "\n",
        "\n",
        "\n",
        "def calculate_perplexity(prompts, model, tokenizer):\n",
        "    total_prompts = len(prompts)\n",
        "    total_perplexity = 0\n",
        "\n",
        "    for prompt in prompts:\n",
        "        textual_prompt = numerical_to_text_prompt[prompt]\n",
        "        inputs = tokenizer(textual_prompt, return_tensors=\"pt\", padding=True, truncation=True)\n",
        "        input_ids = inputs.input_ids.to(model.device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            outputs = model(input_ids, labels=input_ids)\n",
        "            loss = outputs.loss\n",
        "\n",
        "        perplexity = torch.exp(loss)\n",
        "        total_perplexity += perplexity.item()\n",
        "\n",
        "    average_perplexity = total_perplexity / total_prompts\n",
        "    return average_perplexity\n",
        "\n",
        "# Load Mistral model and tokenizer\n",
        "model_name = \"microsoft/DialoGPT-small\"\n",
        "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "# Add a padding token to the tokenizer\n",
        "tokenizer.add_special_tokens({'pad_token': '[PAD]'})  # Add a new padding token\n",
        "\n",
        "# Define numerical prompts and corresponding references\n",
        "prompts = [0, 1]  # Numerical prompts\n",
        "references = [0, 1]  # Numerical references\n",
        "\n",
        "# Calculate metrics using Mistral model\n",
        "accuracy = calculate_accuracy(prompts, references, model, tokenizer)\n",
        "f1_score = calculate_f1_score(prompts, references, model, tokenizer)\n",
        "mape = calculate_mape(prompts, references, model, tokenizer)\n",
        "perplexity = calculate_perplexity(prompts, model, tokenizer)\n",
        "\n",
        "# Print results\n",
        "print(f\"Accuracy: {accuracy:.2f}%\")\n",
        "print(f\"F1 Score: {f1_score:.2f}\")\n",
        "print(f\"MAPE: {mape:.2f}\")\n",
        "print(f\"Perplexity: {perplexity:.2f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZpQYXunbL1-p",
        "outputId": "c25bc0f3-0962-4b8e-8af3-d697e9107513"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 50.00%\n",
            "F1 Score: 0.50\n",
            "MAPE: 0.00\n",
            "Perplexity: 84103.03\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "now please mention me the further steps\n",
        "\n",
        "HEllo sir, are you there ?"
      ],
      "metadata": {
        "id": "O2jEZxWYb6i1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "target_accuracy = 80.0\n",
        "target_f1_score = 0.70\n",
        "target_mape = 0.20\n",
        "target_perplexity = 50.0"
      ],
      "metadata": {
        "id": "wOvVMiOWbh8k"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Print results and evaluate against target values\n",
        "print(f\"Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\")\n",
        "print(f\"Accuracy: {accuracy:.2f}%\")\n",
        "print(f\"F1 Score: {f1_score:.2f}\")\n",
        "print(f\"MAPE: {mape:.2f}\")\n",
        "print(f\"Perplexity: {perplexity:.2f}\")\n",
        "\n",
        "# Evaluate against target values\n",
        "print(\"\\nTarget Evaluation:\")\n",
        "print(f\"Accuracy: {'PASS' if accuracy >= target_accuracy else 'FAIL'} (Target: {target_accuracy}%)\")\n",
        "print(f\"F1 Score: {'PASS' if f1_score >= target_f1_score else 'FAIL'} (Target: {target_f1_score})\")\n",
        "print(f\"MAPE: {'PASS' if mape <= target_mape else 'FAIL'} (Target: {target_mape})\")\n",
        "print(f\"Perplexity: {'PASS' if perplexity <= target_perplexity else 'FAIL'} (Target: {target_perplexity})\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 228
        },
        "id": "sRQJhz62GC4D",
        "outputId": "0904f241-7ac6-486f-aa8c-8fd15c4931c5"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'accuracy' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-edccaf9899cc>\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Print results and evaluate against target values\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Accuracy: {accuracy:.2f}%\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"F1 Score: {f1_score:.2f}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"MAPE: {mape:.2f}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'accuracy' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "from sklearn.metrics import f1_score as sklearn_f1_score  # Rename the imported function\n",
        "\n",
        "# Define your functions here (generate_response, calculate_accuracy, etc.)\n",
        "\n",
        "# Load different Mistral models and tokenizers\n",
        "model_name1 = \"microsoft/DialoGPT-small\"\n",
        "model_name2 = \"microsoft/DialoGPT-medium\"\n",
        "tokenizer1 = AutoTokenizer.from_pretrained(model_name1)\n",
        "tokenizer2 = AutoTokenizer.from_pretrained(model_name2)\n",
        "model1 = AutoModelForCausalLM.from_pretrained(model_name1)\n",
        "model2 = AutoModelForCausalLM.from_pretrained(model_name2)\n",
        "\n",
        "# Add a padding token to the tokenizers\n",
        "tokenizer1.add_special_tokens({'pad_token': '[PAD]'})\n",
        "tokenizer2.add_special_tokens({'pad_token': '[PAD]'})\n",
        "\n",
        "# Define numerical prompts and corresponding references\n",
        "prompts = [0, 1]  # Numerical prompts\n",
        "references = [0, 1]  # Numerical references\n",
        "\n",
        "# Calculate metrics using Mistral models\n",
        "accuracy1 = calculate_accuracy(prompts, references, model1, tokenizer1)\n",
        "accuracy2 = calculate_accuracy(prompts, references, model2, tokenizer2)\n",
        "\n",
        "f1_score1 = calculate_f1_score(prompts, references, model1, tokenizer1)\n",
        "f1_score2 = calculate_f1_score(prompts, references, model2, tokenizer2)\n",
        "\n",
        "mape1 = calculate_mape(prompts, references, model1, tokenizer1)\n",
        "mape2 = calculate_mape(prompts, references, model2, tokenizer2)\n",
        "\n",
        "perplexity1 = calculate_perplexity(prompts, model1, tokenizer1)\n",
        "perplexity2 = calculate_perplexity(prompts, model2, tokenizer2)\n",
        "\n",
        "# Print results for Mistral models\n",
        "print(f\"Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\")\n",
        "print(\"Metrics for Mistral DialoGPT-small:\")\n",
        "print(f\"Accuracy: {accuracy1:.2f}%\")\n",
        "print(f\"F1 Score: {f1_score1:.2f}\")\n",
        "print(f\"MAPE: {mape1:.2f}\")\n",
        "print(f\"Perplexity: {perplexity1:.2f}\")\n",
        "\n",
        "print(\"\\nMetrics for Mistral DialoGPT-medium:\")\n",
        "print(f\"Accuracy: {accuracy2:.2f}%\")\n",
        "print(f\"F1 Score: {f1_score2:.2f}\")\n",
        "print(f\"MAPE: {mape2:.2f}\")\n",
        "print(f\"Perplexity: {perplexity2:.2f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 560
        },
        "id": "ofsOmjOrGI28",
        "outputId": "7b502f52-fc07-4f47-9154-9e5879b148b2"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "'numpy.float64' object is not callable",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-28-bc26af0f0c6f>\u001b[0m in \u001b[0;36m<cell line: 27>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0maccuracy2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcalculate_accuracy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprompts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreferences\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenizer2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m \u001b[0mf1_score1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcalculate_f1_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprompts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreferences\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenizer1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m \u001b[0mf1_score2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcalculate_f1_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprompts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreferences\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenizer2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-23-c417274342f1>\u001b[0m in \u001b[0;36mcalculate_f1_score\u001b[0;34m(prompts, references, model, tokenizer)\u001b[0m\n\u001b[1;32m     47\u001b[0m         \u001b[0mall_references\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtextual_reference\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m     \u001b[0mf1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf1_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_references\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mall_predicted\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maverage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'micro'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mf1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: 'numpy.float64' object is not callable"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**I am lagging too much and not able to point to the code, lets take rest for few hours, what do you think sir?\n",
        "fine. **"
      ],
      "metadata": {
        "id": "zkCCx8smZ38v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "lets meet 2 hours sharp from now."
      ],
      "metadata": {
        "id": "fhR5cAKOH-Nc"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}