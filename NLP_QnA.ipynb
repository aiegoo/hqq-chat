{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyO9SAshweKn/e7wbx7YuRsK",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aiegoo/hqq-chat/blob/master/NLP_QnA.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1XZw6ysCmvI2"
      },
      "outputs": [],
      "source": [
        "!pip install datasets"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import datasets\n",
        "accuracy_metric = datasets.load_metric('accuracy')"
      ],
      "metadata": {
        "id": "kvh69w9um6hN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "results = accuracy_metric.compute(references=[0, 1], predictions=[0, 1])"
      ],
      "metadata": {
        "id": "VDVq8EL_nR0r"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "results"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9CciQbCwnbCy",
        "outputId": "e399923a-dde5-4266-db4f-ca8c77ab31e8"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'accuracy': 1.0}"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "results = accuracy_metric.compute(references=[0, 1, 2, 0, 1, 2], predictions=[0, 1, 1, 2, 1, 0])"
      ],
      "metadata": {
        "id": "GhyOPbwFncEc"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "results"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x1qi1kMznjz-",
        "outputId": "1921b590-0963-4533-98cf-a44c1125a4ab"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'accuracy': 0.5}"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "results = accuracy_metric.compute(references=[0, 1, 2, 0, 1, 2], predictions=[0, 1, 1, 2, 1, 0], normalize=False)"
      ],
      "metadata": {
        "id": "M7k5WY3-nk8h"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "results"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_vmHIqNYnvee",
        "outputId": "55d934ec-f6fe-4b61-b7d0-073db7f5b043"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'accuracy': 3.0}"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "results = accuracy_metric.compute(references=[0, 1, 2, 0, 1, 2], predictions=[0, 1, 1, 2, 1, 0], sample_weight=[0.5, 2, 0.7, 0.5, 9, 0.4])"
      ],
      "metadata": {
        "id": "yTL3R399nxAY"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "results"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bRLuMF64oB6b",
        "outputId": "5ce5dacc-346e-44f4-ec5d-09dd7f415b55"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'accuracy': 0.8778625954198473}"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**LLM Mixtral**"
      ],
      "metadata": {
        "id": "Pw92NMPXpreY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers torch"
      ],
      "metadata": {
        "id": "N_bJ8cQlosif"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForCausalLM, AutoTokenizer"
      ],
      "metadata": {
        "id": "7EChcj7Muvd_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def calculate_accuracy(prompts, references, model, tokenizer):\n",
        "    total_prompts = len(prompts)\n",
        "    correct_predictions = 0\n",
        "\n",
        "    for prompt, reference in zip(prompts, references):\n",
        "        # Map numerical prompt to textual representation\n",
        "        textual_prompt = numerical_to_text_prompt[prompt]\n",
        "\n",
        "        # Tokenize the textual prompt with padding\n",
        "        inputs = tokenizer(textual_prompt, return_tensors=\"pt\", padding=True, truncation=True)\n",
        "        input_ids = inputs.input_ids.to(model.device)\n",
        "\n",
        "        # Generate a response using Mistral\n",
        "        generated_ids = model.generate(input_ids, max_length=50, num_return_sequences=1)\n",
        "        generated_response = tokenizer.decode(generated_ids[0], skip_special_tokens=True)\n",
        "\n",
        "        # Map numerical reference to textual representation\n",
        "        textual_reference = numerical_to_text_response[reference]\n",
        "\n",
        "        # Check if the generated response matches the reference\n",
        "        if generated_response == textual_reference:\n",
        "            correct_predictions += 1\n",
        "\n",
        "    # Calculate accuracy\n",
        "    accuracy = (correct_predictions / total_prompts) * 100\n",
        "    return accuracy\n",
        "\n",
        "# Define mapping from numerical values to textual prompts/responses\n",
        "numerical_to_text_prompt = {\n",
        "    0: \"A for Apple\",\n",
        "    1: \"B for Banana\"\n",
        "}\n",
        "\n",
        "numerical_to_text_response = {\n",
        "    0: \"A for Apple\",\n",
        "    1: \"B for Banana\"\n",
        "}\n",
        "\n",
        "# Load Mistral model and tokenizer\n",
        "model = AutoModelForCausalLM.from_pretrained(\"microsoft/DialoGPT-small\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/DialoGPT-small\")\n",
        "\n",
        "# Set padding token in the tokenizer\n",
        "tokenizer.add_special_tokens({'pad_token': '[PAD]'})  # Add a new padding token\n",
        "\n",
        "# Define numerical prompts and corresponding references\n",
        "prompts = [0, 1]  # Numerical prompts\n",
        "references = [0, 1]  # Numerical references\n",
        "\n",
        "# Calculate accuracy using Mistral model\n",
        "accuracy = calculate_accuracy(prompts, references, model, tokenizer)\n",
        "print(f\"Accuracy: {accuracy:.2f}%\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aPacMWAmwsqu",
        "outputId": "0476603d-5c18-4eb0-e836-acd23e58a1fd"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 50.00%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_accuracy(prompts, references, model, tokenizer):\n",
        "    total_prompts = len(prompts)\n",
        "    correct_predictions = 0\n",
        "\n",
        "    for prompt, reference in zip(prompts, references):\n",
        "        # Map numerical prompt to textual representation\n",
        "        textual_prompt = numerical_to_text_prompt[prompt]\n",
        "\n",
        "        # Tokenize the textual prompt with padding\n",
        "        inputs = tokenizer(textual_prompt, return_tensors=\"pt\", padding=True, truncation=True)\n",
        "        input_ids = inputs.input_ids.to(model.device)\n",
        "\n",
        "        # Generate a response using Mistral\n",
        "        generated_ids = model.generate(input_ids, max_length=50, num_return_sequences=1)\n",
        "        generated_response = tokenizer.decode(generated_ids[0], skip_special_tokens=True)\n",
        "\n",
        "        # Map numerical reference to textual representation\n",
        "        textual_reference = numerical_to_text_response[reference]\n",
        "\n",
        "        # Check if the generated response matches the reference\n",
        "        if generated_response == textual_reference:\n",
        "            correct_predictions += 1\n",
        "\n",
        "    # Calculate accuracy\n",
        "    accuracy = (correct_predictions / total_prompts) * 100\n",
        "    return accuracy\n",
        "\n",
        "# Define mapping from numerical values to textual prompts/responses\n",
        "numerical_to_text_prompt = {\n",
        "    0: \"A for Apple\",\n",
        "    1: \"B for Banana\",\n",
        "    2: \"C for Cherry\"\n",
        "\n",
        "}\n",
        "\n",
        "numerical_to_text_response = {\n",
        "    0: \"A for Apple\",\n",
        "    1: \"B for Banana\",\n",
        "    2: \"C for Cherry\"\n",
        "}\n",
        "\n",
        "# Load Mistral model and tokenizer\n",
        "model = AutoModelForCausalLM.from_pretrained(\"microsoft/DialoGPT-small\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/DialoGPT-small\")\n",
        "\n",
        "# Set padding token in the tokenizer\n",
        "tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
        "accuracy = calculate_accuracy([0, 1, 2, 0, 1, 2], [0, 1, 1, 2, 1, 0], model, tokenizer)\n",
        "print(f\"Accuracy: {accuracy:.2f}%\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KB-SILztyQ0E",
        "outputId": "06e94cf1-4ca2-40ad-e224-24b68a9c59eb"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 33.33%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for prompt, reference in zip(prompts, references):\n",
        "    textual_prompt = numerical_to_text_prompt[prompt]\n",
        "    inputs = tokenizer(textual_prompt, return_tensors=\"pt\", padding=True, truncation=True)\n",
        "    input_ids = inputs.input_ids.to(model.device)\n",
        "\n",
        "    # Generate a response using Mistral\n",
        "    generated_ids = model.generate(input_ids, max_length=50, num_return_sequences=1)\n",
        "    generated_response = tokenizer.decode(generated_ids[0], skip_special_tokens=True)\n",
        "\n",
        "    print(f\"Prompt: {textual_prompt}\")\n",
        "    print(f\"Expected Response: {numerical_to_text_response[reference]}\")\n",
        "    print(f\"Generated Response: {generated_response}\\n\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mzDeNmvZqJrP",
        "outputId": "8de5e8c1-ebe0-4682-f170-568824d4d139"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prompt: A for Apple\n",
            "Expected Response: A for Apple\n",
            "Generated Response: A for Apple Watch\n",
            "\n",
            "Prompt: B for Banana\n",
            "Expected Response: B for Banana\n",
            "Generated Response: B for Banana\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "accuracy = calculate_accuracy([0, 1, 2, 0, 1, 2], [0, 1, 1, 2, 1, 0], model, tokenizer)\n",
        "print(f\"Accuracy: {accuracy:.2f}%\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rQdKLEUCsKfn",
        "outputId": "55b4e358-a6c5-401f-d665-5646cf877b71"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 33.33%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**HEllo sir, are we going good? I will cook food now, okay sir?**"
      ],
      "metadata": {
        "id": "pU2LHTGL04S1"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "AAu4P-6jsKc8"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "5dg0hHTssKZy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "> import llm model mixtral\n",
        "> run the metrics on them"
      ],
      "metadata": {
        "id": "puwv9gS1o1ej"
      }
    }
  ]
}