# -*- coding: utf-8 -*-
"""mistral_metrics.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1YN_WrT9Kq4fPjrwus_936KK3Vyqfy32v
"""

!pip install nltk evaluate sklearn tqdm

import os
import random
import functools
import csv
import numpy as np
import torch
import torch.nn.functional as F
from datasets import load_dataset
from sklearn.metrics import f1_score
from datasets import Dataset, DatasetDict
import tqdm
from peft import (
    LoraConfig,
    prepare_model_for_kbit_training,
    get_peft_model,PeftModel
)
from transformers import (
    AutoModelForSequenceClassification,
    AutoModelForCausalLM,
    AutoTokenizer,
    BitsAndBytesConfig,
    TrainingArguments,Trainer)

import nltk
from nltk.translate.bleu_score import corpus_bleu, SmoothingFunction
from nltk.tokenize import word_tokenize
from nltk.lm import MLE, Vocabulary
from nltk.util import everygrams
from nltk.lm.preprocessing import padded_everygram_pipeline
from nltk.corpus import reuters
import evaluate
import datasets



model_name = "mistralai/Mistral-7B-v0.1"
tokenizer = AutoTokenizer.from_pretrained(model_name, add_eos_token=True, use_fast=True)
tokenizer.pad_token = tokenizer.eos_token
tokenizer.padding_side = 'left'

dataset = load_dataset("uconcreative/slmDataset")

compute_dtype = getattr(torch, "bfloat16")
bnb_config = BitsAndBytesConfig(
        load_in_4bit=True,
        bnb_4bit_quant_type="nf4",
        bnb_4bit_compute_dtype=compute_dtype,
        bnb_4bit_use_double_quant=True,
)
model = AutoModelForCausalLM.from_pretrained(
          model_name, quantization_config=bnb_config, device_map={"": 0},  attn_implementation="flash_attention_2"
)

model = prepare_model_for_kbit_training(model)
model.config.pad_token_id = tokenizer.pad_token_id
peft_config = LoraConfig(
        lora_alpha=16,
        lora_dropout=0.05,
        r=16,
        bias="none",
        task_type="CAUSAL_LM",
        target_modules= ['k_proj', 'q_proj', 'v_proj', 'o_proj', "gate_proj", "down_proj", "up_proj"]
)


training_arguments = TrainingArguments(
        output_dir="./v2_mistral7b_results",
        evaluation_strategy="steps",
        do_eval=True,
        optim="paged_adamw_8bit",
        per_device_train_batch_size=12,
        gradient_accumulation_steps=2,
        per_device_eval_batch_size=12,
        log_level="debug",
        logging_steps=100,
        learning_rate=1e-4,
        eval_steps=25,
        max_steps=100,
        save_steps=25,
        warmup_steps=25,
        lr_scheduler_type="linear",
)
trainer = Trainer(
        model=model,
        train_dataset=dataset['train'],
        #eval_dataset=dataset['test'],
        peft_config=peft_config,
        dataset_text_field="text",
        max_seq_length=1024,
        tokenizer=tokenizer,
        args=training_arguments,
)

trainer.train()

from transformers import GenerationConfig

model.config.use_cache = True
model = PeftModel.from_pretrained(model, "./results/checkpoint-100/")


def generate(instruction):
    prompt = "### Human: " + instruction + "### Assistant: "
    inputs = tokenizer(prompt, return_tensors="pt")
    input_ids = inputs["input_ids"].cuda()
    generation_output = model.generate(
        input_ids=input_ids,
        generation_config=GenerationConfig(pad_token_id=tokenizer.pad_token_id, temperature=1.0, top_p=1.0, top_k=50,
                                           num_beams=1),
        return_dict_in_generate=True,
        output_scores=True,


def compute_metrics(p):
    predictions, labels = p
    f1_micro = f1_score(labels, predictions > 0, average = 'micro')
    f1_macro = f1_score(labels, predictions > 0, average = 'macro')
    f1_weighted = f1_score(labels, predictions > 0, average = 'weighted')
    return {
        'f1_micro': f1_micro,
        'f1_macro': f1_macro,
        'f1_weighted': f1_weighted
    }

def bleu_calc(predictions):
    references = [[word_tokenize(reuters.raw()[15000:20000])]]
    bleu_score = corpus_bleu(references, predictions, smoothing_function=SmoothingFunction().method7)
    return bleu_score

#perplexity
perplexity = evaluate.load("perplexity", module_type="metric")
input_texts = datasets.load_dataset("wikitext",
                                    "wikitext-2-raw-v1",
                                    split="test")["text"][:50]
input_texts = [s for s in input_texts if s!='']
results = perplexity.compute(model_id=model,
                             predictions=input_texts)
